{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oQfvT7X2fUob"
   },
   "source": [
    "#**Operaciones de Machine Learning**\n",
    "#**Proyecto 1**: Crear un ambiente de desarrollo de machine learning en el cual sea posible la ingesta, validaci´on y transformación de datos.\n",
    "#**Grupo: 5**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KetS6oc9C0sR"
   },
   "source": [
    "Importar librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "da3bWGg4Cvyx",
    "outputId": "f4a0f519-58db-4842-ada7-63760eefd59a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-02 02:12:27.346346: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-03-02 02:12:27.371022: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:479] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-03-02 02:12:27.393010: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:10575] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-03-02 02:12:27.393053: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1442] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-03-02 02:12:27.419088: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF version: 2.16.2\n",
      "TFDV version: 1.16.1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "import numpy as np\n",
    "import tfx\n",
    "import tensorflow_transform as tft\n",
    "from tfx.orchestration.experimental.interactive.interactive_context import InteractiveContext\n",
    "from tfx.components import CsvExampleGen,StatisticsGen,SchemaGen,ExampleValidator,Transform\n",
    "from tfx.v1.components import ImportSchemaGen\n",
    "import tensorflow_data_validation as tfdv\n",
    "from tensorflow_metadata.proto.v0 import schema_pb2,anomalies_pb2\n",
    "from google.protobuf import text_format\n",
    "from tfx.orchestration.metadata import sqlite_metadata_connection_config\n",
    "from ml_metadata.metadata_store import MetadataStore\n",
    "from ml_metadata.proto import metadata_store_pb2\n",
    "print('TF version:', tf.__version__)\n",
    "print('TFDV version:', tfdv.version.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lx_X-XfzC9KJ"
   },
   "source": [
    "#Cargar Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 226
    },
    "id": "T00jEotCCwvv",
    "outputId": "5d1bb383-8aad-4272-a2a7-445e0ec1d2d6"
   },
   "outputs": [],
   "source": [
    " ## download the dataset\n",
    " # Directory of the raw data files\n",
    " _data_root = '/data/covertype'\n",
    " # Path to the raw training data\n",
    " _data_filepath = os.path.join(_data_root, 'covertype_test.csv')\n",
    " # Download data\n",
    " os.makedirs(_data_root, exist_ok=True)\n",
    " if not os.path.isfile(_data_filepath):\n",
    "     #https://archive.ics.uci.edu/ml/machine-learning-databases/covtype/\n",
    "     url = 'https://docs.google.com/uc?export= \\\n",
    "     download&confirm={{VALUE}}&id=1lVF1BCWLH4eXXV_YOJzjR7xZjj-wAGj9'\n",
    "     r = requests.get(url, allow_redirects=True, stream=True)\n",
    "     open(_data_filepath, 'wb').write(r.content)\n",
    "# Cargar el dataset\n",
    "df = pd.read_csv(_data_filepath)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1cFkkSwfFSG0"
   },
   "source": [
    "#Selección de Caracteristicas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U7K2aBhXEOMb",
    "outputId": "8d993aa1-72ae-4102-e361-f0b0cfa39cc8"
   },
   "outputs": [],
   "source": [
    "# Separar la variable objetivo\n",
    "target = 'Cover_Type'\n",
    "numerical_features = df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "\n",
    "# Eliminar la variable objetivo de la lista de características\n",
    "numerical_features.remove(target)\n",
    "\n",
    "# Crear el subconjunto solo con las características numéricas\n",
    "df_numerical = df[numerical_features + [target]]\n",
    "\n",
    "print(\"Características numéricas:\", numerical_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5x3fM1qwFoiX",
    "outputId": "baac3bc4-58a1-4d35-8039-7b52913344ff"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Definir variables de entrada (X) y salida (y)\n",
    "X = df_numerical.drop(columns=[target])\n",
    "y = df_numerical[target]\n",
    "\n",
    "k = 8  # Número de características a seleccionar\n",
    "selector = SelectKBest(score_func=f_classif, k=k)\n",
    "X_new = selector.fit_transform(X, y)\n",
    "\n",
    "# Obtener las características seleccionadas\n",
    "selected_features = np.array(numerical_features)[selector.get_support()]\n",
    "print(\"Características seleccionadas:\", selected_features.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear un nuevo DataFrame con solo las características seleccionadas\n",
    "df_selected = df[selected_features.tolist()]\n",
    "\n",
    "# Agregar la columna 'Cover_Type' de nuevo al DataFrame\n",
    "df_selected[target] = df[target]\n",
    "\n",
    "# Guardar el DataFrame en el archivo original, reemplazando el archivo CSV\n",
    "df_selected.to_csv(_data_filepath, index=False)\n",
    "\n",
    "print(f\"El archivo con las características seleccionadas ha sido guardado en: {_data_filepath}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UfpmInceG1KO"
   },
   "source": [
    "#Data Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zW4BjAWxG2lf"
   },
   "outputs": [],
   "source": [
    "#4.1 Configurar contexto interactivo\n",
    "\n",
    "# Definir directorio de metadatos\n",
    "_pipeline_root = '/data/tfx_pipeline'\n",
    "_metadata_path = os.path.join(_pipeline_root, 'metadata.db')\n",
    "\n",
    "# Crear contexto interactivo\n",
    "from tfx.orchestration.metadata import sqlite_metadata_connection_config\n",
    "\n",
    "context = InteractiveContext(\n",
    "    pipeline_root=_pipeline_root,\n",
    "    metadata_connection_config=sqlite_metadata_connection_config(os.path.join(_pipeline_root, 'metadata.db'))\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8ujKoMVrNPLr"
   },
   "outputs": [],
   "source": [
    "#4.2 ExampleGen\n",
    "\n",
    "# Crear componente ExampleGen\n",
    "example_gen = CsvExampleGen(input_base=_data_root)\n",
    "\n",
    "# Ejecutar componente\n",
    "context.run(example_gen)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4.3 Estadisticas\n",
    "\n",
    "# Crear y ejecutar el componente StatisticsGen\n",
    "statistics_gen = StatisticsGen(examples=example_gen.outputs['examples'])\n",
    "context.run(statistics_gen)\n",
    "\n",
    "print(\" StatisticsGen ejecutado correctamente.\")\n",
    "\n",
    "\n",
    "# Obtener la ruta del directorio de estadísticas\n",
    "statistics_uri = statistics_gen.outputs['statistics'].get()[0].uri\n",
    "\n",
    "# Rutas de los archivos FeatureStats.pb en Split-train y Split-eval\n",
    "train_stats_path = os.path.join(statistics_uri, 'Split-train', 'FeatureStats.pb')\n",
    "eval_stats_path = os.path.join(statistics_uri, 'Split-eval', 'FeatureStats.pb')\n",
    "\n",
    "# Verificar si los archivos existen\n",
    "if not os.path.exists(train_stats_path) or not os.path.exists(eval_stats_path):\n",
    "    raise FileNotFoundError(f\" No se encontraron los archivos FeatureStats.pb en {statistics_uri}\")\n",
    "\n",
    "print(f\" Archivo de estadísticas de entrenamiento: {train_stats_path}\")\n",
    "print(f\" Archivo de estadísticas de evaluación: {eval_stats_path}\")\n",
    "\n",
    "# Cargar estadísticas desde los archivos .pb\n",
    "train_stats = tfdv.load_stats_binary(train_stats_path)\n",
    "eval_stats = tfdv.load_stats_binary(eval_stats_path)\n",
    "\n",
    "# Visualizar estadísticas comparando entrenamiento y evaluación\n",
    "tfdv.visualize_statistics(lhs_statistics=train_stats, rhs_statistics=eval_stats, lhs_name=\"Train\", rhs_name=\"Eval\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4.4 Inferir el esquema\n",
    "\n",
    "\n",
    "# Ejecutar SchemaGen\n",
    "schema_gen = SchemaGen(statistics=statistics_gen.outputs['statistics'])\n",
    "context.run(schema_gen)\n",
    "\n",
    "# Obtener la ruta del esquema generado\n",
    "schema_uri = schema_gen.outputs['schema'].get()[0].uri\n",
    "print(f\" Esquema generado en: {schema_uri}\")\n",
    "\n",
    "# Verificar archivos generados\n",
    "print(f\" Contenido de {schema_uri}: {os.listdir(schema_uri)}\")\n",
    "\n",
    "# Cargar y visualizar el esquema\n",
    "schema = tfdv.load_schema_text(os.path.join(schema_uri, 'schema.pbtxt'))\n",
    "tfdv.display_schema(schema)\n",
    "\n",
    "# Inspeccionar características inferidas\n",
    "for feature in schema.feature:\n",
    "    print(f\" Feature: {feature.name}\")\n",
    "    print(f\"   Tipo: {feature.type}\")\n",
    "    print(f\"   Presencia: {feature.presence.min_fraction if feature.HasField('presence') else 'N/A'}\")\n",
    "    print(\"-\" * 40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4.5 Curando el esquema\n",
    "\n",
    "\n",
    "# Ejecutar SchemaGen\n",
    "schema_gen = SchemaGen(statistics=statistics_gen.outputs['statistics'])\n",
    "context.run(schema_gen)\n",
    "\n",
    "# Cargar el esquema inferido\n",
    "schema_uri = schema_gen.outputs['schema'].get()[0].uri\n",
    "schema_path = os.path.join(schema_uri, \"schema.pbtxt\")\n",
    "schema = tfdv.load_schema_text(schema_path)\n",
    "\n",
    "# Definir rangos permitidos para variables numéricas\n",
    "tfdv.set_domain(schema, \"Hillshade_9am\", schema_pb2.IntDomain(min=0, max=255))\n",
    "tfdv.set_domain(schema, \"Hillshade_Noon\", schema_pb2.IntDomain(min=0, max=255))\n",
    "tfdv.set_domain(schema, \"Slope\", schema_pb2.IntDomain(min=0, max=90))\n",
    "\n",
    "# Definir Cover Type como categórica\n",
    "cover_type_domain = schema_pb2.IntDomain(min=0, max=6)\n",
    "cover_type_feature = next(f for f in schema.feature if f.name == \"Cover_Type\")\n",
    "cover_type_feature.int_domain.CopyFrom(cover_type_domain)\n",
    "cover_type_feature.type = schema_pb2.FeatureType.INT  # Mantener como INT\n",
    "cover_type_feature.annotation.tag.append(\"categorical\")  # Marcar como categórica\n",
    "\n",
    "# Guardar el esquema actualizado\n",
    "schema_output_path = os.path.join(schema_uri, \"schema.pbtxt\")\n",
    "tfdv.write_schema_text(schema, schema_output_path)\n",
    "\n",
    "# Verificar cambios\n",
    "schema_updated = tfdv.load_schema_text(schema_output_path)\n",
    "tfdv.display_schema(schema_updated)\n",
    "\n",
    "print(f\" Esquema actualizado guardado en: {schema_output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.6 Entornos de Esquema\n",
    "\n",
    "#  Directorios de datos\n",
    "train_data_path = \"/data/covertype/train\"\n",
    "inference_data_path = \"/data/covertype/inference\"\n",
    "\n",
    "#  Crear directorios si no existen\n",
    "os.makedirs(train_data_path, exist_ok=True)\n",
    "os.makedirs(inference_data_path, exist_ok=True)\n",
    "\n",
    "#  Cargar datos de entrenamiento y generar datos de inferencia\n",
    "train_file = \"/data/covertype/covertype_test.csv\"\n",
    "df_train = pd.read_csv(train_file)\n",
    "\n",
    "#  Crear datos de inferencia (eliminando Cover_Type)\n",
    "df_inference = df_train.drop(columns=[\"Cover_Type\"])\n",
    "df_inference.to_csv(f\"{inference_data_path}/covertype_inference.csv\", index=False)\n",
    "\n",
    "#  Mover el archivo de entrenamiento a su carpeta correspondiente\n",
    "df_train.to_csv(f\"{train_data_path}/covertype_test.csv\", index=False)\n",
    "\n",
    "#  Cargar el esquema generado en 4.4\n",
    "schema_uri = schema_gen.outputs['schema'].get()[0].uri\n",
    "schema_path = os.path.join(schema_uri, \"schema.pbtxt\")\n",
    "schema = tfdv.load_schema_text(schema_path)\n",
    "\n",
    "#  Evitar duplicados en los entornos\n",
    "if \"TRAINING\" not in schema.default_environment:\n",
    "    schema.default_environment.append(\"TRAINING\")\n",
    "if \"SERVING\" not in schema.default_environment:\n",
    "    schema.default_environment.append(\"SERVING\")\n",
    "\n",
    "# Excluir Cover_Type en SERVING sin duplicados\n",
    "for feature in schema.feature:\n",
    "    if feature.name == \"Cover_Type\" and \"SERVING\" not in feature.not_in_environment:\n",
    "        feature.not_in_environment.append(\"SERVING\")\n",
    "\n",
    "# Guardar el esquema actualizado\n",
    "schema_env_path = os.path.join(schema_uri, \"schema.pbtxt\")\n",
    "tfdv.write_schema_text(schema, schema_env_path)\n",
    "\n",
    "# Verificar configuración de entornos\n",
    "print(\" Entornos en el esquema:\", schema.default_environment)\n",
    "for feature in schema.feature:\n",
    "    if feature.name == \"Cover_Type\":\n",
    "        print(f\" Feature: {feature.name}\")\n",
    "        print(f\"   Excluido del entorno: {feature.not_in_environment}\")\n",
    "\n",
    "#  Crear ExampleGen para los datos de entrenamiento e inferencia\n",
    "train_example_gen = CsvExampleGen(input_base=train_data_path)\n",
    "context.run(train_example_gen)\n",
    "\n",
    "inference_example_gen = CsvExampleGen(input_base=inference_data_path)\n",
    "context.run(inference_example_gen)\n",
    "\n",
    "#  Generar estadísticas para datos de inferencia\n",
    "inference_stats_gen = StatisticsGen(examples=inference_example_gen.outputs['examples'])\n",
    "context.run(inference_stats_gen)\n",
    "\n",
    "# Validar datos de inferencia usando el esquema actualizado\n",
    "example_validator = ExampleValidator(\n",
    "    statistics=inference_stats_gen.outputs['statistics'],\n",
    "    schema=schema_gen.outputs['schema']\n",
    ")\n",
    "\n",
    "context.run(example_validator)\n",
    "print(\" Validación de datos de inferencia completada.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Verificación entornos\n",
    "\n",
    "\n",
    "#  Cargar el esquema guardado\n",
    "schema_env_path = os.path.join(schema_gen.outputs['schema'].get()[0].uri, \"schema.pbtxt\")\n",
    "schema = tfdv.load_schema_text(schema_env_path)\n",
    "\n",
    "#  Verificar entornos disponibles\n",
    "print(\" Entornos en el esquema:\", schema.default_environment)\n",
    "\n",
    "#  Verificar que Cover_Type está excluido en SERVING\n",
    "for feature in schema.feature:\n",
    "    if feature.name == \"Cover_Type\":\n",
    "        print(f\" Feature: {feature.name}\")\n",
    "        print(f\"   Excluido del entorno: {feature.not_in_environment}\")\n",
    "\n",
    "#  Mostrar rangos de las columnas en Domain\n",
    "print(\"\\n Rango de valores permitidos en las características numéricas:\")\n",
    "for feature in schema.feature:\n",
    "    if feature.HasField(\"int_domain\"):  # Verificar si la columna tiene un dominio de valores\n",
    "        print(f\" {feature.name}: {feature.int_domain.min} - {feature.int_domain.max}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4.7 Nuevas Estadisticas\n",
    "\n",
    "#  Ruta del esquema actualizado\n",
    "schema_env_path = os.path.join(schema_gen.outputs['schema'].get()[0].uri, \"schema.pbtxt\")\n",
    "\n",
    "#  Crear componente ImportSchemaGen para importar el esquema al pipeline\n",
    "schema_importer = ImportSchemaGen(schema_file=schema_env_path)\n",
    "\n",
    "#  Ejecutar ImportSchemaGen\n",
    "context.run(schema_importer)\n",
    "print(\" Esquema actualizado importado con ImportSchemaGen.\")\n",
    "\n",
    "#  Generar nuevas estadísticas con el esquema curado\n",
    "updated_stats_gen = StatisticsGen(\n",
    "    examples=example_gen.outputs['examples'],  # Usar los datos originales\n",
    "    schema=schema_importer.outputs['schema']  # Pasar el esquema actualizado\n",
    ")\n",
    "\n",
    "#  Ejecutar StatisticsGen con el nuevo esquema\n",
    "context.run(updated_stats_gen)\n",
    "print(\" Nuevas estadísticas generadas con el esquema actualizado.\")\n",
    "\n",
    "#  Ruta al archivo de estadísticas dentro de Split-train\n",
    "stats_file = os.path.join(updated_stats_gen.outputs['statistics'].get()[0].uri, \"Split-train\", \"FeatureStats.pb\")\n",
    "\n",
    "#  Cargar las estadísticas desde el archivo binario\n",
    "updated_stats = tfdv.load_stats_binary(stats_file)\n",
    "\n",
    "#  Visualizar estadísticas actualizadas\n",
    "tfdv.visualize_statistics(updated_stats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4.8 Comprobar anomalias \n",
    "\n",
    "# Ejecutar ExampleValidator con las estadísticas y el esquema actualizados\n",
    "example_validator = ExampleValidator(\n",
    "    statistics=updated_stats_gen.outputs['statistics'],  # Estadísticas generadas en 4.7\n",
    "    schema=schema_importer.outputs['schema']  # Esquema importado con ImportSchemaGen\n",
    ")\n",
    "\n",
    "# Ejecutar el componente para detectar anomalías\n",
    "context.run(example_validator)\n",
    "print(\"Comprobación de anomalías completada.\")\n",
    "\n",
    "# Ruta del directorio de anomalías generadas\n",
    "anomalies_dir = example_validator.outputs['anomalies'].get()[0].uri\n",
    "print(\"Ruta de salida de anomalías:\", anomalies_dir)\n",
    "\n",
    "# Verificar archivos dentro del directorio de anomalías\n",
    "print(\"Contenido en Split-train:\", os.listdir(os.path.join(anomalies_dir, \"Split-train\")))\n",
    "print(\"Contenido en Split-eval:\", os.listdir(os.path.join(anomalies_dir, \"Split-eval\")))\n",
    "\n",
    "# Ruta al archivo de diferencias detectadas dentro de Split-train\n",
    "anomalies_file = os.path.join(anomalies_dir, \"Split-train\", \"SchemaDiff.pb\")\n",
    "\n",
    "# Verificar si el archivo de anomalías existe\n",
    "if os.path.exists(anomalies_file):\n",
    "    # Cargar el archivo SchemaDiff.pb como Protobuf\n",
    "    anomalies_proto = anomalies_pb2.Anomalies()\n",
    "    with open(anomalies_file, \"rb\") as f:\n",
    "        anomalies_proto.ParseFromString(f.read())\n",
    "\n",
    "    # Inspeccionar los campos del objeto anomalies_proto\n",
    "    print(\"Estructura de anomalías (campos disponibles):\")\n",
    "    print(anomalies_proto)  # Esto muestra los campos disponibles en el objeto\n",
    "\n",
    "    # Intentar ver la representación en texto del objeto completo\n",
    "    print(\"Representación de las anomalías en formato de texto:\")\n",
    "    print(text_format.MessageToString(anomalies_proto))\n",
    "else:\n",
    "    print(f\"El archivo {anomalies_file} no existe.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Ingeneria de Caracteristicas - Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear el componente Transform\n",
    "transform = Transform(\n",
    "    examples=example_gen.outputs[\"examples\"],  # Datos de entrada (del paso ExampleGen)\n",
    "    schema=schema_importer.outputs[\"schema\"],  # Esquema curado (del paso SchemaGen o ImportSchemaGen)\n",
    "    module_file=\"/tfx/notebooks/transform_module.py\"  # Archivo con la función de transformación\n",
    ")\n",
    "\n",
    "# Ejecutar el componente Transform\n",
    "context.run(transform)\n",
    "\n",
    "# Obtener la salida de las características transformadas\n",
    "transform_graph_uri = transform.outputs[\"transform_graph\"].get()[0].uri\n",
    "transformed_examples_uri = transform.outputs[\"transformed_examples\"].get()[0].uri\n",
    "\n",
    "print(f\"Transformaciones completadas. Gráfica de transformación guardada en: {transform_graph_uri}\")\n",
    "print(f\"Datos transformados guardados en: {transformed_examples_uri}\")\n",
    "\n",
    "# Verificar archivos generados en el directorio de datos transformados\n",
    "tfrecord_files = tf.io.gfile.glob(os.path.join(transformed_examples_uri, \"Split-train/*\"))\n",
    "\n",
    "# Asegurar que se generaron archivos correctamente\n",
    "if not tfrecord_files:\n",
    "    raise FileNotFoundError(f\"No se encontraron archivos TFRecord en {transformed_examples_uri}/Split-train\")\n",
    "\n",
    "# Manejo de archivos comprimidos\n",
    "dataset = tf.data.TFRecordDataset(tfrecord_files, compression_type=\"GZIP\")\n",
    "\n",
    "# Leer algunos ejemplos transformados para verificar los cambios\n",
    "print(\"\\nEjemplos transformados (primeros 5 registros):\")\n",
    "for raw_record in dataset.take(5):\n",
    "    example = tf.train.Example()\n",
    "    example.ParseFromString(raw_record.numpy())\n",
    "    print(example)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Metadatos de aprendizaje automático"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usar la misma ruta de metadatos que en el pipeline\n",
    "METADATA_PATH = \"/data/tfx_pipeline/metadata.db\"  # Asegurar que es metadata.db y no metadata.sqlite\n",
    "\n",
    "# Crear una conexión con el almacén de metadatos\n",
    "metadata_config = sqlite_metadata_connection_config(METADATA_PATH)\n",
    "store = MetadataStore(metadata_config)\n",
    "\n",
    "print(f\" Conexión establecida con el almacén de metadatos en {METADATA_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def listar_tipos_de_artefactos():\n",
    "    \"\"\"Lista todos los tipos de artefactos almacenados en ML Metadata con sus IDs.\"\"\"\n",
    "    artifact_types = store.get_artifact_types()\n",
    "    \n",
    "    print(\"\\n Tipos de artefactos almacenados en el pipeline:\")\n",
    "    for artifact in artifact_types:\n",
    "        print(f\"- ID: {artifact.id}, Nombre: {artifact.name}\")\n",
    "\n",
    "# Ejecutar la función\n",
    "listar_tipos_de_artefactos()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtener_artefactos_por_tipo(tipo_artefacto):\n",
    "    \"\"\"Lista los artefactos de un tipo específico y sus propiedades.\"\"\"\n",
    "    artifacts = store.get_artifacts_by_type(tipo_artefacto)\n",
    "\n",
    "    print(f\"\\n Artefactos de tipo: {tipo_artefacto}\")\n",
    "    for artifact in artifacts:\n",
    "        # Obtener eventos asociados al artefacto\n",
    "        events = store.get_events_by_artifact_ids([artifact.id])\n",
    "        \n",
    "        # Filtrar eventos de salida (que indican qué componente generó el artefacto)\n",
    "        producer_execution_id = None\n",
    "        for event in events:\n",
    "            if event.type == metadata_store_pb2.Event.OUTPUT:\n",
    "                producer_execution_id = event.execution_id\n",
    "                break  # Tomamos solo el primer evento de salida\n",
    "\n",
    "        # Obtener el nombre del componente productor\n",
    "        producer_component = \"Desconocido\"\n",
    "        if producer_execution_id:\n",
    "            execution = store.get_executions_by_id([producer_execution_id])[0]\n",
    "            producer_component = execution.properties[\"pipeline_step\"].string_value\n",
    "\n",
    "        print(f\"- ID: {artifact.id}, URI: {artifact.uri}, Estado: {artifact.state}, Creado por: {producer_component}\")\n",
    "\n",
    "# Obtener información sobre los esquemas almacenados\n",
    "obtener_artefactos_por_tipo(\"Schema\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtener_propiedades_artefacto(artifact_id):\n",
    "    \"\"\"Muestra todas las propiedades de un artefacto en particular, incluyendo el componente que lo generó.\"\"\"\n",
    "    artifact = store.get_artifacts_by_id([artifact_id])[0]\n",
    "\n",
    "    print(f\"\\n Propiedades del artefacto ID {artifact_id}:\")\n",
    "    print(f\"- Tipo ID: {artifact.type_id}\")\n",
    "    print(f\"- URI: {artifact.uri}\")\n",
    "    print(f\"- Estado: {artifact.state}\")\n",
    "\n",
    "    # Obtener eventos asociados al artefacto\n",
    "    events = store.get_events_by_artifact_ids([artifact.id])\n",
    "    \n",
    "    # Filtrar eventos de salida (que indican qué componente generó el artefacto)\n",
    "    producer_execution_id = None\n",
    "    for event in events:\n",
    "        if event.type == metadata_store_pb2.Event.OUTPUT:\n",
    "            producer_execution_id = event.execution_id\n",
    "            break  # Tomamos solo el primer evento de salida\n",
    "\n",
    "    # Obtener el nombre del componente productor\n",
    "    producer_component = \"Desconocido\"\n",
    "    if producer_execution_id:\n",
    "        execution = store.get_executions_by_id([producer_execution_id])[0]\n",
    "        producer_component = execution.properties[\"pipeline_step\"].string_value\n",
    "\n",
    "    print(f\"- Creado por: {producer_component}\")\n",
    "\n",
    "    # Mostrar propiedades adicionales\n",
    "    for key, value in artifact.custom_properties.items():\n",
    "        print(f\"- {key}: {value}\")\n",
    "\n",
    "# Obtener detalles de un Schema específico (por ejemplo, ID=5)\n",
    "obtener_propiedades_artefacto(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtener_artefactos_entrada(artifact_ids):\n",
    "    \"\"\"Devuelve los artefactos de entrada utilizados para generar una lista de artefactos.\"\"\"\n",
    "\n",
    "    if not isinstance(artifact_ids, list):\n",
    "        artifact_ids = [artifact_ids]  # Convertir en lista si es un solo ID\n",
    "\n",
    "    artifact_list = store.get_artifacts_by_id(artifact_ids)\n",
    "\n",
    "    if not artifact_list:\n",
    "        print(f\"No se encontraron artefactos con los IDs {artifact_ids}\")\n",
    "        return\n",
    "\n",
    "    for artifact in artifact_list:\n",
    "        print(f\"\\n Buscando artefactos de entrada para: Tipo {artifact.type_id} (ID {artifact.id})\")\n",
    "        print(f\"URI: {artifact.uri}\")\n",
    "\n",
    "        # Obtener eventos en los que este artefacto fue una salida\n",
    "        events = store.get_events_by_artifact_ids([artifact.id])\n",
    "\n",
    "        # Obtener las ejecuciones que generaron este artefacto\n",
    "        executions = [\n",
    "            event.execution_id for event in events if event.type == metadata_store_pb2.Event.OUTPUT\n",
    "        ]\n",
    "\n",
    "        if not executions:\n",
    "            print(\" No se encontraron ejecuciones asociadas a este artefacto.\")\n",
    "            continue\n",
    "\n",
    "        print(f\" Generado en las ejecuciones: {executions}\")\n",
    "\n",
    "        # Obtener eventos de entrada en esas ejecuciones\n",
    "        execution_events = store.get_events_by_execution_ids(executions)\n",
    "\n",
    "        # Filtrar eventos que representen entradas\n",
    "        input_artifacts = list({\n",
    "            event.artifact_id for event in execution_events if event.type == metadata_store_pb2.Event.INPUT\n",
    "        })  # Convertir a lista para evitar duplicados\n",
    "\n",
    "        if not input_artifacts:\n",
    "            print(\"No se encontraron artefactos de entrada para este artefacto.\")\n",
    "            continue\n",
    "\n",
    "        print(\"\\n Artefactos utilizados como entrada:\")\n",
    "        for input_artifact_id in input_artifacts:\n",
    "            input_artifact_list = store.get_artifacts_by_id([input_artifact_id])\n",
    "            if input_artifact_list:\n",
    "                input_artifact = input_artifact_list[0]\n",
    "                print(f\"- ID: {input_artifact.id}, Tipo: {input_artifact.type_id}, URI: {input_artifact.uri}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener artefactos de entrada para múltiples artefactos\n",
    "obtener_artefactos_entrada([2, 3, 5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
