{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oQfvT7X2fUob"
   },
   "source": [
    "#**Operaciones de Machine Learning**\n",
    "#**Proyecto 1**: Crear un ambiente de desarrollo de machine learning en el cual sea posible la ingesta, validaci´on y transformación de datos.\n",
    "#**Grupo: 5**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KetS6oc9C0sR"
   },
   "source": [
    "Importar librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "da3bWGg4Cvyx",
    "outputId": "f4a0f519-58db-4842-ada7-63760eefd59a"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "import numpy as np\n",
    "import tfx\n",
    "from tfx.orchestration.experimental.interactive.interactive_context import InteractiveContext\n",
    "from tfx.components import CsvExampleGen,StatisticsGen,SchemaGen,ExampleValidator\n",
    "from tfx.v1.components import ImportSchemaGen\n",
    "import tensorflow_data_validation as tfdv\n",
    "from tensorflow_metadata.proto.v0 import schema_pb2,anomalies_pb2\n",
    "from google.protobuf import text_format\n",
    "print('TF version:', tf.__version__)\n",
    "print('TFDV version:', tfdv.version.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lx_X-XfzC9KJ"
   },
   "source": [
    "#Cargar Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 226
    },
    "id": "T00jEotCCwvv",
    "outputId": "5d1bb383-8aad-4272-a2a7-445e0ec1d2d6"
   },
   "outputs": [],
   "source": [
    " ## download the dataset\n",
    " # Directory of the raw data files\n",
    " _data_root = '/data/covertype'\n",
    " # Path to the raw training data\n",
    " _data_filepath = os.path.join(_data_root, 'covertype_test.csv')\n",
    " # Download data\n",
    " os.makedirs(_data_root, exist_ok=True)\n",
    " if not os.path.isfile(_data_filepath):\n",
    "     #https://archive.ics.uci.edu/ml/machine-learning-databases/covtype/\n",
    "     url = 'https://docs.google.com/uc?export= \\\n",
    "     download&confirm={{VALUE}}&id=1lVF1BCWLH4eXXV_YOJzjR7xZjj-wAGj9'\n",
    "     r = requests.get(url, allow_redirects=True, stream=True)\n",
    "     open(_data_filepath, 'wb').write(r.content)\n",
    "# Cargar el dataset\n",
    "df = pd.read_csv(_data_filepath)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1cFkkSwfFSG0"
   },
   "source": [
    "#Selección de Caracteristicas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U7K2aBhXEOMb",
    "outputId": "8d993aa1-72ae-4102-e361-f0b0cfa39cc8"
   },
   "outputs": [],
   "source": [
    "# Separar la variable objetivo\n",
    "target = 'Cover_Type'\n",
    "numerical_features = df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "\n",
    "# Eliminar la variable objetivo de la lista de características\n",
    "numerical_features.remove(target)\n",
    "\n",
    "# Crear el subconjunto solo con las características numéricas\n",
    "df_numerical = df[numerical_features + [target]]\n",
    "\n",
    "print(\"Características numéricas:\", numerical_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5x3fM1qwFoiX",
    "outputId": "baac3bc4-58a1-4d35-8039-7b52913344ff"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Definir variables de entrada (X) y salida (y)\n",
    "X = df_numerical.drop(columns=[target])\n",
    "y = df_numerical[target]\n",
    "\n",
    "k = 8  # Número de características a seleccionar\n",
    "selector = SelectKBest(score_func=f_classif, k=k)\n",
    "X_new = selector.fit_transform(X, y)\n",
    "\n",
    "# Obtener las características seleccionadas\n",
    "selected_features = np.array(numerical_features)[selector.get_support()]\n",
    "print(\"Características seleccionadas:\", selected_features.tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UfpmInceG1KO"
   },
   "source": [
    "#Data Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zW4BjAWxG2lf"
   },
   "outputs": [],
   "source": [
    "#4.1 Configurar contexto interactivo\n",
    "\n",
    "# Definir directorio de metadatos\n",
    "_pipeline_root = '/data/tfx_pipeline'\n",
    "_metadata_path = os.path.join(_pipeline_root, 'metadata.db')\n",
    "\n",
    "# Crear contexto interactivo\n",
    "from tfx.orchestration.metadata import sqlite_metadata_connection_config\n",
    "\n",
    "context = InteractiveContext(\n",
    "    pipeline_root=_pipeline_root,\n",
    "    metadata_connection_config=sqlite_metadata_connection_config(os.path.join(_pipeline_root, 'metadata.db'))\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8ujKoMVrNPLr"
   },
   "outputs": [],
   "source": [
    "#4.2 ExampleGen\n",
    "\n",
    "# Crear componente ExampleGen\n",
    "example_gen = CsvExampleGen(input_base=_data_root)\n",
    "\n",
    "# Ejecutar componente\n",
    "context.run(example_gen)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4.3 Estadisticas\n",
    "\n",
    "# Crear y ejecutar el componente StatisticsGen\n",
    "statistics_gen = StatisticsGen(examples=example_gen.outputs['examples'])\n",
    "context.run(statistics_gen)\n",
    "\n",
    "print(\" StatisticsGen ejecutado correctamente.\")\n",
    "\n",
    "\n",
    "# Obtener la ruta del directorio de estadísticas\n",
    "statistics_uri = statistics_gen.outputs['statistics'].get()[0].uri\n",
    "\n",
    "# Rutas de los archivos FeatureStats.pb en Split-train y Split-eval\n",
    "train_stats_path = os.path.join(statistics_uri, 'Split-train', 'FeatureStats.pb')\n",
    "eval_stats_path = os.path.join(statistics_uri, 'Split-eval', 'FeatureStats.pb')\n",
    "\n",
    "# Verificar si los archivos existen\n",
    "if not os.path.exists(train_stats_path) or not os.path.exists(eval_stats_path):\n",
    "    raise FileNotFoundError(f\" No se encontraron los archivos FeatureStats.pb en {statistics_uri}\")\n",
    "\n",
    "print(f\" Archivo de estadísticas de entrenamiento: {train_stats_path}\")\n",
    "print(f\" Archivo de estadísticas de evaluación: {eval_stats_path}\")\n",
    "\n",
    "# Cargar estadísticas desde los archivos .pb\n",
    "train_stats = tfdv.load_stats_binary(train_stats_path)\n",
    "eval_stats = tfdv.load_stats_binary(eval_stats_path)\n",
    "\n",
    "# Visualizar estadísticas comparando entrenamiento y evaluación\n",
    "tfdv.visualize_statistics(lhs_statistics=train_stats, rhs_statistics=eval_stats, lhs_name=\"Train\", rhs_name=\"Eval\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4.4 Inferir el esquema\n",
    "\n",
    "\n",
    "# Ejecutar SchemaGen\n",
    "schema_gen = SchemaGen(statistics=statistics_gen.outputs['statistics'])\n",
    "context.run(schema_gen)\n",
    "\n",
    "# Obtener la ruta del esquema generado\n",
    "schema_uri = schema_gen.outputs['schema'].get()[0].uri\n",
    "print(f\" Esquema generado en: {schema_uri}\")\n",
    "\n",
    "# Verificar archivos generados\n",
    "print(f\" Contenido de {schema_uri}: {os.listdir(schema_uri)}\")\n",
    "\n",
    "# Cargar y visualizar el esquema\n",
    "schema = tfdv.load_schema_text(os.path.join(schema_uri, 'schema.pbtxt'))\n",
    "tfdv.display_schema(schema)\n",
    "\n",
    "# Inspeccionar características inferidas\n",
    "for feature in schema.feature:\n",
    "    print(f\" Feature: {feature.name}\")\n",
    "    print(f\"   Tipo: {feature.type}\")\n",
    "    print(f\"   Presencia: {feature.presence.min_fraction if feature.HasField('presence') else 'N/A'}\")\n",
    "    print(\"-\" * 40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4.5 Curando el esquema\n",
    "\n",
    "\n",
    "# Ejecutar SchemaGen\n",
    "schema_gen = SchemaGen(statistics=statistics_gen.outputs['statistics'])\n",
    "context.run(schema_gen)\n",
    "\n",
    "# Cargar el esquema inferido\n",
    "schema_uri = schema_gen.outputs['schema'].get()[0].uri\n",
    "schema_path = os.path.join(schema_uri, \"schema.pbtxt\")\n",
    "schema = tfdv.load_schema_text(schema_path)\n",
    "\n",
    "# Definir rangos permitidos para variables numéricas\n",
    "tfdv.set_domain(schema, \"Hillshade_9am\", schema_pb2.IntDomain(min=0, max=255))\n",
    "tfdv.set_domain(schema, \"Hillshade_Noon\", schema_pb2.IntDomain(min=0, max=255))\n",
    "tfdv.set_domain(schema, \"Slope\", schema_pb2.IntDomain(min=0, max=90))\n",
    "\n",
    "# Definir Cover Type como categórica\n",
    "cover_type_domain = schema_pb2.IntDomain(min=0, max=6)\n",
    "cover_type_feature = next(f for f in schema.feature if f.name == \"Cover_Type\")\n",
    "cover_type_feature.int_domain.CopyFrom(cover_type_domain)\n",
    "cover_type_feature.type = schema_pb2.FeatureType.INT  # Mantener como INT\n",
    "cover_type_feature.annotation.tag.append(\"categorical\")  # Marcar como categórica\n",
    "\n",
    "# Guardar el esquema actualizado\n",
    "schema_output_path = os.path.join(schema_uri, \"schema.pbtxt\")\n",
    "tfdv.write_schema_text(schema, schema_output_path)\n",
    "\n",
    "# Verificar cambios\n",
    "schema_updated = tfdv.load_schema_text(schema_output_path)\n",
    "tfdv.display_schema(schema_updated)\n",
    "\n",
    "print(f\" Esquema actualizado guardado en: {schema_output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.6 Entornos de Esquema\n",
    "\n",
    "#  Directorios de datos\n",
    "train_data_path = \"/data/covertype/train\"\n",
    "inference_data_path = \"/data/covertype/inference\"\n",
    "\n",
    "#  Crear directorios si no existen\n",
    "os.makedirs(train_data_path, exist_ok=True)\n",
    "os.makedirs(inference_data_path, exist_ok=True)\n",
    "\n",
    "#  Cargar datos de entrenamiento y generar datos de inferencia\n",
    "train_file = \"/data/covertype/covertype_test.csv\"\n",
    "df_train = pd.read_csv(train_file)\n",
    "\n",
    "#  Crear datos de inferencia (eliminando Cover_Type)\n",
    "df_inference = df_train.drop(columns=[\"Cover_Type\"])\n",
    "df_inference.to_csv(f\"{inference_data_path}/covertype_inference.csv\", index=False)\n",
    "\n",
    "#  Mover el archivo de entrenamiento a su carpeta correspondiente\n",
    "df_train.to_csv(f\"{train_data_path}/covertype_test.csv\", index=False)\n",
    "\n",
    "#  Cargar el esquema generado en 4.4\n",
    "schema_uri = schema_gen.outputs['schema'].get()[0].uri\n",
    "schema_path = os.path.join(schema_uri, \"schema.pbtxt\")\n",
    "schema = tfdv.load_schema_text(schema_path)\n",
    "\n",
    "#  Evitar duplicados en los entornos\n",
    "if \"TRAINING\" not in schema.default_environment:\n",
    "    schema.default_environment.append(\"TRAINING\")\n",
    "if \"SERVING\" not in schema.default_environment:\n",
    "    schema.default_environment.append(\"SERVING\")\n",
    "\n",
    "# Excluir Cover_Type en SERVING sin duplicados\n",
    "for feature in schema.feature:\n",
    "    if feature.name == \"Cover_Type\" and \"SERVING\" not in feature.not_in_environment:\n",
    "        feature.not_in_environment.append(\"SERVING\")\n",
    "\n",
    "# Guardar el esquema actualizado\n",
    "schema_env_path = os.path.join(schema_uri, \"schema.pbtxt\")\n",
    "tfdv.write_schema_text(schema, schema_env_path)\n",
    "\n",
    "# Verificar configuración de entornos\n",
    "print(\" Entornos en el esquema:\", schema.default_environment)\n",
    "for feature in schema.feature:\n",
    "    if feature.name == \"Cover_Type\":\n",
    "        print(f\" Feature: {feature.name}\")\n",
    "        print(f\"   Excluido del entorno: {feature.not_in_environment}\")\n",
    "\n",
    "#  Crear ExampleGen para los datos de entrenamiento e inferencia\n",
    "train_example_gen = CsvExampleGen(input_base=train_data_path)\n",
    "context.run(train_example_gen)\n",
    "\n",
    "inference_example_gen = CsvExampleGen(input_base=inference_data_path)\n",
    "context.run(inference_example_gen)\n",
    "\n",
    "#  Generar estadísticas para datos de inferencia\n",
    "inference_stats_gen = StatisticsGen(examples=inference_example_gen.outputs['examples'])\n",
    "context.run(inference_stats_gen)\n",
    "\n",
    "# Validar datos de inferencia usando el esquema actualizado\n",
    "example_validator = ExampleValidator(\n",
    "    statistics=inference_stats_gen.outputs['statistics'],\n",
    "    schema=schema_gen.outputs['schema']\n",
    ")\n",
    "\n",
    "context.run(example_validator)\n",
    "print(\" Validación de datos de inferencia completada.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Verificación entornos\n",
    "\n",
    "\n",
    "#  Cargar el esquema guardado\n",
    "schema_env_path = os.path.join(schema_gen.outputs['schema'].get()[0].uri, \"schema.pbtxt\")\n",
    "schema = tfdv.load_schema_text(schema_env_path)\n",
    "\n",
    "#  Verificar entornos disponibles\n",
    "print(\" Entornos en el esquema:\", schema.default_environment)\n",
    "\n",
    "#  Verificar que Cover_Type está excluido en SERVING\n",
    "for feature in schema.feature:\n",
    "    if feature.name == \"Cover_Type\":\n",
    "        print(f\" Feature: {feature.name}\")\n",
    "        print(f\"   Excluido del entorno: {feature.not_in_environment}\")\n",
    "\n",
    "#  Mostrar rangos de las columnas en Domain\n",
    "print(\"\\n Rango de valores permitidos en las características numéricas:\")\n",
    "for feature in schema.feature:\n",
    "    if feature.HasField(\"int_domain\"):  # Verificar si la columna tiene un dominio de valores\n",
    "        print(f\" {feature.name}: {feature.int_domain.min} - {feature.int_domain.max}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4.7 Nuevas Estadisticas\n",
    "\n",
    "#  Ruta del esquema actualizado\n",
    "schema_env_path = os.path.join(schema_gen.outputs['schema'].get()[0].uri, \"schema.pbtxt\")\n",
    "\n",
    "#  Crear componente ImportSchemaGen para importar el esquema al pipeline\n",
    "schema_importer = ImportSchemaGen(schema_file=schema_env_path)\n",
    "\n",
    "#  Ejecutar ImportSchemaGen\n",
    "context.run(schema_importer)\n",
    "print(\" Esquema actualizado importado con ImportSchemaGen.\")\n",
    "\n",
    "#  Generar nuevas estadísticas con el esquema curado\n",
    "updated_stats_gen = StatisticsGen(\n",
    "    examples=example_gen.outputs['examples'],  # Usar los datos originales\n",
    "    schema=schema_importer.outputs['schema']  # Pasar el esquema actualizado\n",
    ")\n",
    "\n",
    "#  Ejecutar StatisticsGen con el nuevo esquema\n",
    "context.run(updated_stats_gen)\n",
    "print(\" Nuevas estadísticas generadas con el esquema actualizado.\")\n",
    "\n",
    "#  Ruta al archivo de estadísticas dentro de Split-train\n",
    "stats_file = os.path.join(updated_stats_gen.outputs['statistics'].get()[0].uri, \"Split-train\", \"FeatureStats.pb\")\n",
    "\n",
    "#  Cargar las estadísticas desde el archivo binario\n",
    "updated_stats = tfdv.load_stats_binary(stats_file)\n",
    "\n",
    "#  Visualizar estadísticas actualizadas\n",
    "tfdv.visualize_statistics(updated_stats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4.8 Comprobar anomalias \n",
    "\n",
    "# Ejecutar ExampleValidator con las estadísticas y el esquema actualizados\n",
    "example_validator = ExampleValidator(\n",
    "    statistics=updated_stats_gen.outputs['statistics'],  # Estadísticas generadas en 4.7\n",
    "    schema=schema_importer.outputs['schema']  # Esquema importado con ImportSchemaGen\n",
    ")\n",
    "\n",
    "# Ejecutar el componente para detectar anomalías\n",
    "context.run(example_validator)\n",
    "print(\"Comprobación de anomalías completada.\")\n",
    "\n",
    "# Ruta del directorio de anomalías generadas\n",
    "anomalies_dir = example_validator.outputs['anomalies'].get()[0].uri\n",
    "print(\"Ruta de salida de anomalías:\", anomalies_dir)\n",
    "\n",
    "# Verificar archivos dentro del directorio de anomalías\n",
    "print(\"Contenido en Split-train:\", os.listdir(os.path.join(anomalies_dir, \"Split-train\")))\n",
    "print(\"Contenido en Split-eval:\", os.listdir(os.path.join(anomalies_dir, \"Split-eval\")))\n",
    "\n",
    "# Ruta al archivo de diferencias detectadas dentro de Split-train\n",
    "anomalies_file = os.path.join(anomalies_dir, \"Split-train\", \"SchemaDiff.pb\")\n",
    "\n",
    "# Verificar si el archivo de anomalías existe\n",
    "if os.path.exists(anomalies_file):\n",
    "    # Cargar el archivo SchemaDiff.pb como Protobuf\n",
    "    anomalies_proto = anomalies_pb2.Anomalies()\n",
    "    with open(anomalies_file, \"rb\") as f:\n",
    "        anomalies_proto.ParseFromString(f.read())\n",
    "\n",
    "    # Inspeccionar los campos del objeto anomalies_proto\n",
    "    print(\"Estructura de anomalías (campos disponibles):\")\n",
    "    print(anomalies_proto)  # Esto muestra los campos disponibles en el objeto\n",
    "\n",
    "    # Intentar ver la representación en texto del objeto completo\n",
    "    print(\"Representación de las anomalías en formato de texto:\")\n",
    "    print(text_format.MessageToString(anomalies_proto))\n",
    "else:\n",
    "    print(f\"El archivo {anomalies_file} no existe.\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
